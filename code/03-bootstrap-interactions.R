#########################################
# Calculate interaction strengths
# (net and emergent)
# boostrap to test significance
# and compare to standard linear models
#
# Tom Smith & Tom Clegg 2022
#########################################

library(tidyverse)
library(cowplot)
library(gtools)

#function that gives all combinations of vector v. 
# n: Total length of possible combinations (i.e. total number of stressors)
# v: Subset of n to choose from (i.e. stressors we want to consider) 
#    Defaults to 1:n where all values are considered. Must be a vector of integers
#
# returns a list of vectors giving each combination ordered by complexity

unique_perms <- function(n, v = 1:n){
  if(!all(v %in% 1:n)){stop("One or more elements of v are not in 1:n")}
  
  #generate matrix with unique permutations
  m <- as.matrix(expand.grid(rep( list(c(0,1)) , n)))
  
  #get combinations from chems
  combinations <- list()
  k <- 1
  #itterate over rows
  for(i in 1:nrow(m)){
    #get elements of v
    itt <- unname(which(m[i,] == 1))
    #check if all of itt is in vector v
    if(all(itt %in% v)){
      combinations[[k]] <- itt   
      k = k+1   
    }
  }
  
  indx <- order(unlist(lapply(combinations,length)))
  
  return(combinations[indx])
}


#function that returns vector of character ids for a given permutation list generated by unique_perms
get_ids <- function(perm){
  ids <- unlist(lapply(perm,function(x){paste(x,collapse = '')}))
  ids[1] <- "control"
  return(ids)
}

### Comparison functions
# Comparison functions take a the various factors required to calculate interaction terms
# and return the ermergent interaction at that level (i.e. the interaction added by including the focal term)
#  focal: measurement of the response in the presence of all stressors
#  control: measurement of response in control (i.e. no stressor)
#  null: vector of all lower interaction terms for all stressors present
#  zero: switch to set the interaction term of the control if TRUE.

#additive
f_additive <- function(focal, control, null, zero = FALSE){
  if(zero){return(0)}
  focal - control - sum(null)
}

#multiplicative
f_multiplicative <- function(focal, control, null, zero = FALSE){
  if(zero){return(1)}
  
  focal  / (control * prod(null))
}

#percent
f_percentchange <- function(focal, control, null, zero = FALSE){
  if(zero){return(0)}
  I = ((focal - control)/ control) - max(-1,sum(null))
  if(abs(I) < 0.01){
    return(0)
  }else{
    return(I)
  }
}

#function that computes the all interation terms for a given combination and response vector r
# n_tot: total number of stressors
# v_int: subset of stressors to consider interactions for. Must be a vector of integers
# r: response vector, named vector of unique measurements for each level of stressor. 
# names must be character strings of stressors present e.g. "12" would be the
# mesurement for stressors 1 and 2. 

calculate_interactions <- function(n_tot, v_int = 1:n_tot, r, func){
  #get all combinations to estimate effects of
  perms <- unique_perms(n_tot,v_int)
  #get unique ids for the effects
  ids <- get_ids(perms)
  
  #named vector to store estimated effects in
  effects <- numeric(length = length(perms))
  names(effects) <- ids
  
  effects[1] <- func(1,1,1,zero = TRUE)
  #loop from low to high complexity
  for(indx in 2:length(effects)){
    complexity <- length(perms[[indx]])
    #if is a single stressor estimate the effects individually
    if(complexity == 1){
      effects[ids[indx]] = func(r[ids[indx]], r["control"], effects[1])
      
      #otherwise build null model from estimated effects
    } else {
      #get permutation leading to current combination (perms[[indx]])
      perm_i <- unique_perms(n_tot, perms[[indx]])
      #get ids of those permutations
      ids_i <- get_ids(perm_i)
      #remove final level, the effect we are estimating
      ids_i <- ids_i[1: (length(ids_i)-1)]
      #caluculate the null effect (summation term) and 
      effects[ids[indx]] <- func(r[ids[indx]], r["control"], effects[ids_i])
    }
  }
  
  return(effects)
}


##########################
# Bootstrapping functions
##########################

#function that checks if a given vector's 90% confidence intervals overlap with 0
check_quantiles <- function(x, t, lb = 0.05, ub = 0.95){
  q <- unname(quantile(x, c(lb,ub)))
  # return(!(q[1] < t & q[2] > t))
  return(TRUE)
}

#function to get a bootstrapped estimate of interaction given:
# r_vec - a vector of observations
# c_vec - a vector of control observations
# i_mat - a N x M matrx of all samples of interaction coefficents that make up the focal effect
# n_samples - number of bootstrap estimates
# zero - switch if the control interaction coeff is returned (1 in this case)
f_mult_bootstrap <- function(r_vec, c_vec, i_mat, n_samples, zero = FALSE){
  if(zero){return(1.0)}
  #generate samples
  r_sample <- sample(r_vec, n_samples, replace = T)
  c_sample <- sample(c_vec, n_samples, replace = T)
  #sample interaction terms for the null model 
  i_mat_perm <- sample(1:ncol(i_mat), n_samples, replace = T)
  i_sample <- i_mat[,i_mat_perm, drop = F]
  
  int_sample <- r_sample / (c_sample * apply(X = i_sample, MARGIN = 2, FUN = prod))
  #if the interaction estimates are far from 0 
  # TS: note this doesn't seem to be doing anything currently
  if(check_quantiles(int_sample, 1.0)){
    return(int_sample)
  } else {
    return(rep(1.0,n_samples))
  }
}

f_min_bootstrap <- function(r_vec, c_vec, i_mat, n_samples, zero = FALSE){
  if(zero){return(1.0)}
  #generate samples
  c_sample <- rep(mean(c_vec), n_samples)
  r_sample <- rep(r_vec[which.min(abs(r_vec - c_vec))], n_samples)
  
  #sample interaction terms for the null model 
  i_mat_perm <- sample(1:ncol(i_mat), n_samples, replace = T)
  i_sample <- i_mat[,i_mat_perm, drop = F]
  
  int_sample <- r_sample / (c_sample * apply(X = i_sample, MARGIN = 2, FUN = prod))
  #if the interaction estimates are far from 0
  if(check_quantiles(int_sample, 1.0)){
    return(int_sample)
  } else {
    return(rep(1.0,n_samples))
  }
}

dominance_bootstrap <- function(r_vec, c_vec, n_samples, zero = FALSE){
  if(zero){return(1.0)}
  #generate samples
  r_sample <- sample(r_vec, n_samples, replace = T)
  c_sample <- sample(c_vec, n_samples, replace = T)
  
  int_sample <- r_sample / c_sample
  # return the sampled values
  return(int_sample)
}


#As above iterate through interaction combinations to determine the effect estimates. returns a matrix with samples of interaction estimates.
calculate_interactions_bootstrap <- function(n_tot, v_int = 1:n_tot, r_list, func, n_samples){
  #get all combinations to estimate effects
  perms <- unique_perms(n_tot,v_int)
  #get unique ids for the effects
  ids <- get_ids(perms)
  #named matrix to store estimated effects in
  effects <- matrix(0,length(perms),n_samples)
  rownames(effects) <- ids
  #set I_0 - the null interaction for the control case (zero = TRUE)
  effects["control",] <- func(1,1,1,n_samples,zero = TRUE)
  
  # loop from low to high complexity
  for(indx in 2:length(ids)){
    complexity <- length(perms[[indx]])
    
    #if is a single stressor estimate the effects individually
    if(complexity == 1){
      effects[ids[indx], ] = func(r_list[[ids[indx]]], r_list[["control"]], effects["control", , drop = F], n_samples = n_samples , zero = FALSE)
      
      
      # otherwise build null model from estimated effects
    } else {
      #get permutation leading to current combination (perms[[indx]])
      perm_i <- unique_perms(n_tot, perms[[indx]])
      #get ids of those permutations
      ids_i <- get_ids(perm_i)
      #remove final level, the effect we are estimating
      ids_i <- ids_i[1: (length(ids_i)-1)]
      #caluculate the interaction effect from 1) the observations at this level 2) the control obs and 3) the estimated effect at lower levels. 
      effects[ids[indx], ] <- func(r_list[[ids[indx]]], r_list[["control"]], effects[ids_i, ,drop = F],n_samples)
    }
  }
  return(effects)
}

# as well as the emergent interactions, we should test the overall net effects of these chem combos
# Hopefully can just alter the previous function to construct a net effect test
calculate_net_effect_bootstrap <- function(n_tot, v_int = 1:n_tot, r_list, func, n_samples){
  #get all combinations to estimate effects
  perms <- unique_perms(n_tot,v_int)
  #get unique ids for the effects
  ids <- get_ids(perms)
  #named matrix to store estimated effects in
  effects <- matrix(0,length(perms),n_samples)
  rownames(effects) <- ids
  #set I_0 - the null interaction for the control case (zero = TRUE)
  effects["control",] <- func(1,1,1,n_samples,zero = TRUE)
  
  # loop from low to high complexity
  for(indx in 2:length(ids)){
    complexity <- length(perms[[indx]])
    
    #if is a single stressor estimate the effects individually
    if(complexity == 1){
      effects[ids[indx], ] = func(r_list[[ids[indx]]], r_list[["control"]], effects["control", , drop = F], n_samples = n_samples , zero = FALSE)
      
      
      # otherwise build null model from estimated effects
    } else {
      # get permutation leading to current combination (perms[[indx]])
      perm_i <- unique_perms(n_tot, perms[[indx]])
      # get ids of only those permutations with length 1 (i.e. drop all the higher order interactions)
      ids_i <- get_ids(perm_i[lengths(perm_i) < 2])
      # calculate the interaction effect from 1) the observations at this level 2) the control obs and 3) the estimated effect at lower levels. 
      effects[ids[indx], ] <- func(r_list[[ids[indx]]], r_list[["control"]], effects[ids_i, ,drop = F],n_samples)
    }
  }
  return(effects)
}

# extra function to calculate a net effect, but allowing for a specified number of extra interactions
calculate_net_effect_bootstrap_plus <- function(n_tot, v_int = 1:n_tot, r_list, func, n_samples, n_interactions){
  #get all combinations to estimate effects
  perms <- unique_perms(n_tot,v_int)
  #get unique ids for the effects
  ids <- get_ids(perms)
  #named matrix to store estimated effects in
  effects <- matrix(0,length(perms),n_samples)
  rownames(effects) <- ids
  #set I_0 - the null interaction for the control case (zero = TRUE)
  effects["control",] <- func(1,1,1,n_samples,zero = TRUE)
  
  # loop from low to high complexity
  for(indx in 2:length(ids)){
    complexity <- length(perms[[indx]])
    
    #if is a single stressor estimate the effects individually
    if(complexity == 1){
      effects[ids[indx], ] = func(r_list[[ids[indx]]], r_list[["control"]], effects["control", , drop = F], n_samples = n_samples , zero = FALSE)
      
      
      # otherwise build null model from estimated effects
    } else {
      # get permutation leading to current combination (perms[[indx]])
      perm_i <- unique_perms(n_tot, perms[[indx]])
      # get ids of only those permutations we're interested in (i.e. drop all the higher order interactions)
      # note that we do need to estimate effects at lower levels, to add a catch based on complexity to stop Infs
      ids_i <- get_ids(perm_i[lengths(perm_i) < min(n_interactions+1, complexity)])
      # calculate the interaction effect from 1) the observations at this level 2) the control obs and 3) the estimated effect at lower levels. 
      effects[ids[indx], ] <- func(r_list[[ids[indx]]], r_list[["control"]], effects[ids_i, ,drop = F],n_samples)
    }
  }
  return(effects)
}

# Extra function for calculating dominance with a bootstrap
calculate_dominance_bootstrap <- function(n_tot, v_int = 1:n_tot, r_list, func, n_samples){
  #get all combinations to estimate effects
  perms <- unique_perms(n_tot,v_int)
  #get unique ids for the effects
  ids <- get_ids(perms)
  #named matrix to store estimated effects in
  effects <- matrix(0,length(perms),n_samples)
  rownames(effects) <- ids
  #set I_0 - the null interaction for the control case (zero = TRUE)
  effects["control",] <- func(1,1,n_samples,zero = TRUE)
  
  # loop from low to high complexity
  for(indx in 2:length(ids)){
    effects[ids[indx], ] = func(r_list[[ids[indx]]], r_list[["control"]], n_samples = n_samples , zero = FALSE)
  }
  return(effects)
}


#######################

# load data with all replicates
grw_df_full <- read_csv("results/spline_fits.csv", show_col_types = FALSE) %>%
  arrange(Strain, Complexity) %>%
  nest(data = c(Amoxicillin, Chlorothalonil, Diflufenican, Glyphosate, Imidacloprid, 
                Metaldehyde, Oxytetracycline, Tebuconazole)) %>%
  mutate(id = unlist(map(data, function(x){ a = paste(which(x == 1),collapse = "")
  return(ifelse(a == "", "control", a))
  })))

options(scipen = 1) 

# create somewhere to put the results
dominance_interactions <- net_interactions <- emergent_interactions <- data.frame()

#apply bootstraped estimation
n_tot <- 8
v_int <- 1:n_tot
n_samples <- 10000

# list of stressors
stressor_list <- c("A", "C", "D", "G", "I", "M", "O", "T")

# apply the bootstrapping interaction functions across all strains
unique_strains <- unique(grw_df_full$Strain)

for(strain in seq_along(unique_strains)){
  print(paste("Bootstrapping strain", unique_strains[strain], sep = " "))
  # subset to the strain we're interested in
  grw_full_subset <- grw_df_full %>% filter(Strain == unique_strains[strain])
  # create list of growth rates with correct names
  grw_nested <- grw_full_subset %>% group_by(id) %>% select(id, AUC) %>% nest 
  
  r_list <- grw_nested %>% pull(data) %>% map(., function(x){x$AUC})
  names(r_list) <- grw_nested$id
  
  # test the emergent interactions
  emergent_mat <- calculate_interactions_bootstrap(n_tot, v_int, r_list, f_mult_bootstrap, n_samples)
  
  # test the net interactions
  net_mat <- calculate_net_effect_bootstrap(n_tot, v_int, r_list, f_mult_bootstrap, n_samples)
  
  # test for effects of dominance
  dom_mat <- calculate_dominance_bootstrap(n_tot, v_int, r_list, dominance_bootstrap, n_samples)
  
    # put all bootstraps into a dataframe
  int_df_emergent <- data_frame(id = rownames(emergent_mat), as.data.frame(emergent_mat)) %>%
    mutate(complexity = ifelse(id == "control",0,nchar(id))) %>%
    pivot_longer(-c(id,complexity), names_to = "rep",values_to = "Interaction") 
  
  int_df_net <- data_frame(id = rownames(net_mat), as.data.frame(net_mat)) %>%
    mutate(complexity = ifelse(id == "control",0,nchar(id))) %>%
    pivot_longer(-c(id,complexity), names_to = "rep",values_to = "Interaction") 
  
  int_df_dom <- data_frame(id = rownames(dom_mat), as.data.frame(dom_mat)) %>%
    mutate(complexity = ifelse(id == "control",0,nchar(id))) %>%
    pivot_longer(-c(id,complexity), names_to = "rep",values_to = "Interaction") 
  
  #####################
  # reformat the data 
  #####################
  
  # --- Emergent first --- #
  
  a <- int_df_emergent %>%
    group_by(id) %>% 
    summarise(mean_int = mean(Interaction), 
              i_min = quantile(Interaction, probs = 0.025),
              i_max = quantile(Interaction, probs = 0.975),
              .groups = "drop")
  
  a <- grw_full_subset %>%
    group_by(id,Strain,Complexity, data) %>%
    summarise(r_min = min(AUC), r_max = max(AUC), r_mean = mean(AUC), .groups = "drop") %>%
    mutate(c_min = ifelse(Complexity == 0, r_min,0),
           c_max = ifelse(Complexity == 0, r_max,0),
           c_mean = ifelse(Complexity == 0, r_mean,0)) %>%
    ungroup %>%
    mutate(c_min = max(c_min), c_max = max(c_max), c_mean = max(c_mean)) %>%
    full_join(a, by = "id") %>%
    arrange(Complexity, id) %>%
    # filter(id == "control") %>%
    mutate(mean_response = r_mean/c_mean) %>%
    select(Strain, data, r_min,r_max,r_mean,c_min,c_max,c_mean, i_min,mean_int, i_max, mean_response, Complexity, id) 
  
  # to test the "significance" of these coefficient estimates,
  # we need to see if the 95% CIs overlap 1.
  a <- a %>%
    rowwise %>% # like a loop - compute a row at-a-time when a vectorised function doesn't exist (i.e. between)
    mutate(significant = ifelse(between(1, i_min, i_max), FALSE, TRUE))

  # define whether a significant interaction is synergistic or antagonistic
  # make it very simple:
  # If the null model of that mixture predicts a negative response, then a -ve interaction coefficient is synergy, +ve is antagonism
  # if the null model is positive, the opposite is true
  
  # first we need to calculate the null model for each row
  a$null <- NA
  
  # loop from low to high complexity
  for(i in 1:nrow(a)){
    model.complexity <- a$Complexity[i]
    
    # if is a single stressor or control, the null is 1
    if(model.complexity < 2){
      a$null[i] <- 1
      # otherwise reconstruct null model from estimated effects
    } else {
      # get the id
      model.id <- a$id[i]
      # get permutation leading to current combination
      null_perms <- unique_perms(8, as.numeric(unlist(str_split(as.numeric(model.id), ""))))
      #get ids of those permutations
      null_ids <- get_ids(null_perms)
      #remove final level, the effect we are estimating, and also control
      null_ids <- null_ids[2: (length(null_ids)-1)]
      #caluculate the null model from the the estimated effects at lower levels. 
      a$null[i] <- prod(a[a$id %in% null_ids,]$mean_int)
    }
  }
  
  # now define the interactions
  # case_when is a good alternative to nested ifelse statements
  
  a <- a %>%
    mutate(response = case_when(Complexity == 0 ~ "None",
                                Complexity == 1 ~ case_when(significant == TRUE ~ ifelse(mean_response > 1, "Positive", "Negative"),
                                                            TRUE ~ "None"),
                                Complexity > 1 ~ case_when(significant == TRUE ~ 
                                                             case_when(null < 1 ~ 
                                                                         ifelse(mean_int < 1, "Synergism", "Antagonism"),
                                                                       null > 1 ~
                                                                         ifelse(mean_int > 1, "Synergism", "Antagonism"),
                                                                       TRUE ~ "categorisation error"),
                                                           TRUE ~ "Additive")))
  
  # and lets un-nest the data columns too
  a <- a %>%
    unnest(data)
  
  
  # --- Repeat for net effects --- #
 
  b <- int_df_net %>%
    group_by(id) %>% 
    summarise(mean_int = mean(Interaction), 
              i_min = quantile(Interaction, probs = 0.025),
              i_max = quantile(Interaction, probs = 0.975),
              .groups = "drop")
  
  b <- grw_full_subset %>%
    group_by(id,Strain,Complexity, data) %>%
    summarise(r_min = min(AUC), r_max = max(AUC), r_mean = mean(AUC), .groups = "drop") %>%
    mutate(c_min = ifelse(Complexity == 0, r_min,0),
           c_max = ifelse(Complexity == 0, r_max,0),
           c_mean = ifelse(Complexity == 0, r_mean,0)) %>%
    ungroup %>%
    mutate(c_min = max(c_min), c_max = max(c_max), c_mean = max(c_mean)) %>%
    full_join(b, by = "id") %>%
    arrange(Complexity, id) %>%
    # filter(id == "control") %>%
    mutate(mean_response = r_mean/c_mean) %>%
    select(Strain, data, r_min,r_max,r_mean,c_min,c_max,c_mean, i_min,mean_int, i_max, mean_response, Complexity, id) 
  
  # test the significance
  b <- b %>%
    rowwise %>% # like a loop - compute a row at-a-time when a vectorised function doesn't exist (i.e. between)
    mutate(significant = ifelse(between(1, i_min, i_max), FALSE, TRUE))
  
  # define interactions
  # first we need to calculate the null model for each row
  b$null <- NA
  
  # loop from low to high complexity
  for(i in 1:nrow(b)){
    model.complexity <- b$Complexity[i]
    
    # if is a single stressor or control, the null is 1
    if(model.complexity < 2){
      b$null[i] <- 1
      # otherwise reconstruct null model from estimated effects
    } else {
      # get the id
      model.id <- b$id[i]
      # get permutation leading to current combination
      null_perms <- unique_perms(8, as.numeric(unlist(str_split(as.numeric(model.id), ""))))
      # get ids of only those permutations with length 1 (i.e. drop all the higher order interactions)
      null_ids <- get_ids(null_perms[lengths(null_perms) < 2])
      null_ids <- null_ids[2: (length(null_ids))]
      #caluculate the null model from the the estimated effects at lower levels. 
      b$null[i] <- prod(b[b$id %in% null_ids,]$mean_int)
    }
  }
  
  # now define the interactions
  # case_when is a good alternative to nested ifelse statements
  
  b <- b %>%
    mutate(response = case_when(Complexity == 0 ~ "None",
                                Complexity == 1 ~ case_when(significant == TRUE ~ ifelse(mean_response > 1, "Positive", "Negative"),
                                                            TRUE ~ "None"),
                                Complexity > 1 ~ case_when(significant == TRUE ~ 
                                                             case_when(null < 1 ~ 
                                                                         ifelse(mean_int < 1, "Synergism", "Antagonism"),
                                                                       null > 1 ~
                                                                         ifelse(mean_int > 1, "Synergism", "Antagonism"),
                                                                       TRUE ~ "categorisation error"),
                                                           TRUE ~ "Additive")))
  
  # and lets un-nest the data columns too
  b <- b %>%
    unnest(data)
  
  
  # # now the tests for dominance
  # 
  # z <- int_df_dom %>%
  #   group_by(id) %>% 
  #   summarise(mean_int = mean(Interaction), 
  #             i_min = quantile(Interaction, probs = 0.025),
  #             i_max = quantile(Interaction, probs = 0.975),
  #             .groups = "drop")
  # 
  # z <- grw_full_subset %>%
  #   group_by(id,Strain,Complexity, data) %>%
  #   summarise(r_min = min(AUC), r_max = max(AUC), r_mean = mean(AUC), .groups = "drop") %>%
  #   mutate(c_min = ifelse(Complexity == 0, r_min,0),
  #          c_max = ifelse(Complexity == 0, r_max,0),
  #          c_mean = ifelse(Complexity == 0, r_mean,0)) %>%
  #   ungroup %>%
  #   mutate(c_min = max(c_min), c_max = max(c_max), c_mean = max(c_mean)) %>%
  #   full_join(z, by = "id") %>%
  #   arrange(Complexity, id) %>%
  #   # filter(id == "control") %>%
  #   mutate(mean_response = r_mean/c_mean) %>%
  #   select(Strain, data, r_min,r_max,r_mean,c_min,c_max,c_mean, i_min,mean_int, i_max, mean_response, Complexity, id) 
  # 
  # # test the significance
  # # more complicated than before because now
  # # we're comparing CIs between single stressors mixtures
  # z$dom_test <- z$dom_chem <- NA
  # 
  # for(i in 1:nrow(z)){
  #   model.complexity <- z$Complexity[i]
  #   
  #   # if is a single stressor or control, we ignore it
  #   if(model.complexity >= 2){
  #     # pull out the id and therefore the component ids
  #     model.id <- z$id[i]
  #     # get permutation leading to current combination
  #     dom_perms <- unique_perms(8, as.numeric(unlist(str_split(as.numeric(model.id), ""))))
  #     # get ids of only those permutations with length 1 (i.e. drop all the higher order interactions)
  #     dom_perms <- dom_perms[lengths(dom_perms) == 1]
  #     dom_ids <- unlist(lapply(dom_perms,function(x){paste(x,collapse = '')}))
  #     
  #     # find the strongest single stressor that we want to compare to
  #     trial_data <- z[z$id %in% dom_ids,]
  #     strongest_id <- trial_data[which.max(abs(1-trial_data$mean_int)),]$id
  #     
  #     # now take the bootstraps from the real data tested and compare CIs to the boostraps from the "strongest ID"
  #     # to test for dominance
  #     # first group the data 
  #     dom_test_dat <- z[z$id %in% c(model.id, strongest_id),]
  #     # then do the CI overlap test
  #     z$dom_test[i] <- max(dom_test_dat$i_min) <= min(dom_test_dat$i_max) # if its TRUE, then the CIs overlap 
  #     # and we have dominance (given a positive net interaction)
  #     # also put in which was the strongest stressor
  #     z$dom_chem[i] <- stressor_list[as.numeric(strongest_id)]
  #     
  #   }
  # }
  # # and lets un-nest the data columns too
  # z <- z %>%
  #   unnest(data)
  
  # co-opting the "dominance" function to just test whether there are differences
  # in the growth from the control, so that we can test whether some of the "additive"
  # responses are actually just no response
  
  z <- int_df_dom %>%
    group_by(id) %>%
    summarise(mean_int = mean(Interaction),
              i_min = quantile(Interaction, probs = 0.025),
              i_max = quantile(Interaction, probs = 0.975),
              .groups = "drop")

  z <- grw_full_subset %>%
    group_by(id,Strain,Complexity, data) %>%
    summarise(r_min = min(AUC), r_max = max(AUC), r_mean = mean(AUC), .groups = "drop") %>%
    mutate(c_min = ifelse(Complexity == 0, r_min,0),
           c_max = ifelse(Complexity == 0, r_max,0),
           c_mean = ifelse(Complexity == 0, r_mean,0)) %>%
    ungroup %>%
    mutate(c_min = max(c_min), c_max = max(c_max), c_mean = max(c_mean)) %>%
    full_join(z, by = "id") %>%
    arrange(Complexity, id) %>%
    # filter(id == "control") %>%
    mutate(mean_response = r_mean/c_mean) %>%
    select(Strain, data, r_min,r_max,r_mean,c_min,c_max,c_mean, i_min,mean_int, i_max, mean_response, Complexity, id)
  
  # test the significance
  z <- z %>%
    rowwise %>% # like a loop - compute a row at-a-time when a vectorised function doesn't exist (i.e. between)
    mutate(significant = ifelse(between(1, i_min, i_max), FALSE, TRUE))
  
  # now define the interactions
  # case_when is a good alternative to nested ifelse statements
  
  z <- z %>%
    mutate(response = case_when(Complexity == 0 ~ "None",
                                Complexity >= 1 ~ case_when(significant == TRUE ~ ifelse(mean_response > 1, "Positive", "Negative"),
                                                            TRUE ~ "None")))
  
  # and lets un-nest the data columns too
  z <- z %>%
    unnest(data)
  
  #############################
  # bind into final dataframes
  #############################
  
  emergent_interactions <- bind_rows(emergent_interactions, a)
  net_interactions <- bind_rows(net_interactions, b)
  dominance_interactions <- bind_rows(dominance_interactions, z)
  
}


# write the results somewhere
write.csv(emergent_interactions, "results/bootstrapped-emergent-interactions.csv", row.names = FALSE)
write.csv(net_interactions, "results/bootstrapped-net-interactions.csv", row.names = FALSE)
write.csv(dominance_interactions, "results/bootstrapped-no-response-test.csv", row.names = FALSE)

#### ---- depreciated ----- #####

# cunningly bind the dominance to the net interactions
# as the dominance is only another option there really
# net_and_dominance <- left_join(net_interactions, dominance_interactions[,c("Strain", "id", "dom_chem", "dom_test")])
# 
# # now do a bit of reworking to finalise what the responses are
# net_and_dominance[net_and_dominance$Complexity > 1 & net_and_dominance$significant == TRUE & 
#                     net_and_dominance$dom_test == TRUE,]$response <- "Dominance"
# 
# write.csv(net_and_dominance, "results/bootstrapped-net-interactions-with-dominance.csv", row.names = FALSE)

#### --------------------- #####

#
#
#
###############################################################################################
# tests for how many interactions we need to incorporate to explain the significant net effects
###############################################################################################
#
#
#


net.2way <- net.3way <- net.4way <- net.5way <- net.6way <- data.frame()

for(strain in seq_along(unique_strains)){
  print(paste("Bootstrapping strain", unique_strains[strain], sep = " "))
  # subset to the strain we're interested in
  grw_full_subset <- grw_df_full %>% filter(Strain == unique_strains[strain])
  # create list of growth rates with correct names
  grw_nested <- grw_full_subset %>% group_by(id) %>% select(id, AUC) %>% nest 
  
  r_list <- grw_nested %>% pull(data) %>% map(., function(x){x$AUC})
  names(r_list) <- grw_nested$id
  
  # sequentially test the net interactions
  net_mat_2way <- calculate_net_effect_bootstrap_plus(n_tot, v_int, r_list, f_mult_bootstrap, n_samples, n_interactions = 2)
  net_mat_3way <- calculate_net_effect_bootstrap_plus(n_tot, v_int, r_list, f_mult_bootstrap, n_samples, n_interactions = 3)
  net_mat_4way <- calculate_net_effect_bootstrap_plus(n_tot, v_int, r_list, f_mult_bootstrap, n_samples, n_interactions = 4)
  net_mat_5way <- calculate_net_effect_bootstrap_plus(n_tot, v_int, r_list, f_mult_bootstrap, n_samples, n_interactions = 5)
  net_mat_6way <- calculate_net_effect_bootstrap_plus(n_tot, v_int, r_list, f_mult_bootstrap, n_samples, n_interactions = 6)

  # put all bootstraps into dataframes
  int_df_2way <- data_frame(id = rownames(net_mat_2way), as.data.frame(net_mat_2way)) %>%
    mutate(complexity = ifelse(id == "control",0,nchar(id))) %>%
    pivot_longer(-c(id,complexity), names_to = "rep",values_to = "Interaction") 
  
  int_df_3way <- data_frame(id = rownames(net_mat_3way), as.data.frame(net_mat_3way)) %>%
    mutate(complexity = ifelse(id == "control",0,nchar(id))) %>%
    pivot_longer(-c(id,complexity), names_to = "rep",values_to = "Interaction") 
  
  int_df_4way <- data_frame(id = rownames(net_mat_4way), as.data.frame(net_mat_4way)) %>%
    mutate(complexity = ifelse(id == "control",0,nchar(id))) %>%
    pivot_longer(-c(id,complexity), names_to = "rep",values_to = "Interaction") 
  
  int_df_5way <- data_frame(id = rownames(net_mat_5way), as.data.frame(net_mat_5way)) %>%
    mutate(complexity = ifelse(id == "control",0,nchar(id))) %>%
    pivot_longer(-c(id,complexity), names_to = "rep",values_to = "Interaction") 
  
  int_df_6way <- data_frame(id = rownames(net_mat_6way), as.data.frame(net_mat_6way)) %>%
    mutate(complexity = ifelse(id == "control",0,nchar(id))) %>%
    pivot_longer(-c(id,complexity), names_to = "rep",values_to = "Interaction") 
  
  #####################
  # reformat the data 
  #####################
  
  # --- 2-way interactions --- #
  
  a <- int_df_2way %>%
    group_by(id) %>% 
    summarise(mean_int = mean(Interaction), 
              i_min = quantile(Interaction, probs = 0.025),
              i_max = quantile(Interaction, probs = 0.975),
              .groups = "drop")
  
  a <- grw_full_subset %>%
    group_by(id,Strain,Complexity, data) %>%
    summarise(r_min = min(AUC), r_max = max(AUC), r_mean = mean(AUC), .groups = "drop") %>%
    mutate(c_min = ifelse(Complexity == 0, r_min,0),
           c_max = ifelse(Complexity == 0, r_max,0),
           c_mean = ifelse(Complexity == 0, r_mean,0)) %>%
    ungroup %>%
    mutate(c_min = max(c_min), c_max = max(c_max), c_mean = max(c_mean)) %>%
    full_join(a, by = "id") %>%
    arrange(Complexity, id) %>%
    # filter(id == "control") %>%
    mutate(mean_response = r_mean/c_mean) %>%
    select(Strain, data, r_min,r_max,r_mean,c_min,c_max,c_mean, i_min,mean_int, i_max, mean_response, Complexity, id) 
  
  # to test the "significance" of these coefficient estimates,
  # we need to see if the 95% CIs overlap 1.
  a <- a %>%
    rowwise %>% # like a loop - compute a row at-a-time when a vectorised function doesn't exist (i.e. between)
    mutate(significant = ifelse(between(1, i_min, i_max), FALSE, TRUE))
  
  # I don't think we need to bother calculating interaction direction here, just want to know if they're still significant
  
  # and lets un-nest the data columns and show how many interactions are in the null too
  a <- a %>%
    mutate(null_model = 2) %>%
    unnest(data)
  
  
  # --- Repeat for 3-way interactions --- #
  
  b <- int_df_3way %>%
    group_by(id) %>% 
    summarise(mean_int = mean(Interaction), 
              i_min = quantile(Interaction, probs = 0.025),
              i_max = quantile(Interaction, probs = 0.975),
              .groups = "drop")
  
  b <- grw_full_subset %>%
    group_by(id,Strain,Complexity, data) %>%
    summarise(r_min = min(AUC), r_max = max(AUC), r_mean = mean(AUC), .groups = "drop") %>%
    mutate(c_min = ifelse(Complexity == 0, r_min,0),
           c_max = ifelse(Complexity == 0, r_max,0),
           c_mean = ifelse(Complexity == 0, r_mean,0)) %>%
    ungroup %>%
    mutate(c_min = max(c_min), c_max = max(c_max), c_mean = max(c_mean)) %>%
    full_join(b, by = "id") %>%
    arrange(Complexity, id) %>%
    # filter(id == "control") %>%
    mutate(mean_response = r_mean/c_mean) %>%
    select(Strain, data, r_min,r_max,r_mean,c_min,c_max,c_mean, i_min,mean_int, i_max, mean_response, Complexity, id) 
  
  # test the significance
  b <- b %>%
    rowwise %>% # like a loop - compute a row at-a-time when a vectorised function doesn't exist (i.e. between)
    mutate(significant = ifelse(between(1, i_min, i_max), FALSE, TRUE))
  
  # and lets un-nest the data columns and show how many interactions are in the null too
  b <- b %>%
    mutate(null_model = 3) %>%
    unnest(data)
  
  # --- Repeat for 4-way interactions --- #
  
  c <- int_df_4way %>%
    group_by(id) %>% 
    summarise(mean_int = mean(Interaction), 
              i_min = quantile(Interaction, probs = 0.025),
              i_max = quantile(Interaction, probs = 0.975),
              .groups = "drop")
  
  c <- grw_full_subset %>%
    group_by(id,Strain,Complexity, data) %>%
    summarise(r_min = min(AUC), r_max = max(AUC), r_mean = mean(AUC), .groups = "drop") %>%
    mutate(c_min = ifelse(Complexity == 0, r_min,0),
           c_max = ifelse(Complexity == 0, r_max,0),
           c_mean = ifelse(Complexity == 0, r_mean,0)) %>%
    ungroup %>%
    mutate(c_min = max(c_min), c_max = max(c_max), c_mean = max(c_mean)) %>%
    full_join(c, by = "id") %>%
    arrange(Complexity, id) %>%
    # filter(id == "control") %>%
    mutate(mean_response = r_mean/c_mean) %>%
    select(Strain, data, r_min,r_max,r_mean,c_min,c_max,c_mean, i_min,mean_int, i_max, mean_response, Complexity, id) 
  
  # test the significance
  c <- c %>%
    rowwise %>% # like a loop - compute a row at-a-time when a vectorised function doesn't exist (i.e. between)
    mutate(significant = ifelse(between(1, i_min, i_max), FALSE, TRUE))
  
  # and lets un-nest the data columns and show how many interactions are in the null too
  c <- c %>%
    mutate(null_model = 4) %>%
    unnest(data)
  
  # --- Repeat for 5-way interactions --- #
  
  d <- int_df_5way %>%
    group_by(id) %>% 
    summarise(mean_int = mean(Interaction), 
              i_min = quantile(Interaction, probs = 0.025),
              i_max = quantile(Interaction, probs = 0.975),
              .groups = "drop")
  
  d <- grw_full_subset %>%
    group_by(id,Strain,Complexity, data) %>%
    summarise(r_min = min(AUC), r_max = max(AUC), r_mean = mean(AUC), .groups = "drop") %>%
    mutate(c_min = ifelse(Complexity == 0, r_min,0),
           c_max = ifelse(Complexity == 0, r_max,0),
           c_mean = ifelse(Complexity == 0, r_mean,0)) %>%
    ungroup %>%
    mutate(c_min = max(c_min), c_max = max(c_max), c_mean = max(c_mean)) %>%
    full_join(d, by = "id") %>%
    arrange(Complexity, id) %>%
    # filter(id == "control") %>%
    mutate(mean_response = r_mean/c_mean) %>%
    select(Strain, data, r_min,r_max,r_mean,c_min,c_max,c_mean, i_min,mean_int, i_max, mean_response, Complexity, id) 
  
  # test the significance
  d <- d %>%
    rowwise %>% # like a loop - compute a row at-a-time when a vectorised function doesn't exist (i.e. between)
    mutate(significant = ifelse(between(1, i_min, i_max), FALSE, TRUE))
  
  # and lets un-nest the data columns and show how many interactions are in the null too
  d <- d %>%
    mutate(null_model = 5) %>%
    unnest(data)
  
  # --- Repeat for 6-way interactions --- #
  
  e <- int_df_6way %>%
    group_by(id) %>% 
    summarise(mean_int = mean(Interaction), 
              i_min = quantile(Interaction, probs = 0.025),
              i_max = quantile(Interaction, probs = 0.975),
              .groups = "drop")
  
  e <- grw_full_subset %>%
    group_by(id,Strain,Complexity, data) %>%
    summarise(r_min = min(AUC), r_max = max(AUC), r_mean = mean(AUC), .groups = "drop") %>%
    mutate(c_min = ifelse(Complexity == 0, r_min,0),
           c_max = ifelse(Complexity == 0, r_max,0),
           c_mean = ifelse(Complexity == 0, r_mean,0)) %>%
    ungroup %>%
    mutate(c_min = max(c_min), c_max = max(c_max), c_mean = max(c_mean)) %>%
    full_join(e, by = "id") %>%
    arrange(Complexity, id) %>%
    # filter(id == "control") %>%
    mutate(mean_response = r_mean/c_mean) %>%
    select(Strain, data, r_min,r_max,r_mean,c_min,c_max,c_mean, i_min,mean_int, i_max, mean_response, Complexity, id) 
  
  # test the significance
  e <- e %>%
    rowwise %>% # like a loop - compute a row at-a-time when a vectorised function doesn't exist (i.e. between)
    mutate(significant = ifelse(between(1, i_min, i_max), FALSE, TRUE))
  
  # and lets un-nest the data columns and show how many interactions are in the null too
  e <- e %>%
    mutate(null_model = 6) %>%
    unnest(data)
  
  #############################
  # bind into final dataframes
  #############################
  
  net.2way <- bind_rows(net.2way, a)
  net.3way <- bind_rows(net.3way, b)
  net.4way <- bind_rows(net.4way, c)
  net.5way <- bind_rows(net.5way, d)
  net.6way <- bind_rows(net.6way, e)
  
}

# write the results somewhere
write.csv(net.2way, "results/bootstrapped-net-interactions-2way.csv", row.names = FALSE)
write.csv(net.3way, "results/bootstrapped-net-interactions-3way.csv", row.names = FALSE)
write.csv(net.4way, "results/bootstrapped-net-interactions-4way.csv", row.names = FALSE)
write.csv(net.5way, "results/bootstrapped-net-interactions-5way.csv", row.names = FALSE)
write.csv(net.6way, "results/bootstrapped-net-interactions-6way.csv", row.names = FALSE)
